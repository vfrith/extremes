'''
27 Jul 2021 

Loading ERA-5 data 
@vikki.thompson
'''

# Load neccessary libraries
import iris
import iris.coord_categorisation as icc
from iris.coord_categorisation import add_season_membership
import numpy as np
import matplotlib.pyplot as plt
import iris.plot as iplt
import cartopy.crs as ccrs
import cartopy as cart
import glob
import matplotlib.cm as mpl_cm
import sys
from iris.experimental.equalise_cubes import equalise_attributes
import scipy.stats as sps
from iris.analysis import geometry

def load_ERA5_day(var, constr):
    '''
    Load in cube of ERA5 daily from BluePebble storage
    var: variable as string, e.g. tasmax, hr, 
    constr: conditions for load
    '''
    file_list = glob.glob('/bp1store/geog-tropical/data/ERA-5/day/'+var+'/*')
    cubes = iris.load(file_list, constr)
    equalise_attributes(cubes) # to allow merge to one cube 
    return cubes.concatenate_cube()

def region_mask(cube, region_value):
    '''
    Masks except region
    '''
    new_cube = cube[0,:,:].copy()
    for i in range(np.shape(cube.data)[1]):
        for j in range(np.shape(cube.data)[2]):
            if cube.coord('regions')[i, j].points == region_value:
                new_cube.data[i,j] = 1
            else:
                new_cube.data[i, j] = np.NaN 
    masked = cube * new_cube
    return masked



############


## Loading a subset of years for testing, just adjust y1/y2 as required
y1 = 2018
y2 = 2018
year_cons = iris.Constraint(time=lambda cell: y1 <= cell.point.year <= y2)
obs = load_ERA5_day('tasmax', year_cons) 
iris.coord_categorisation.add_month_number(obs, 'time', name='month_number')

## Get regions data, and put on obs grid
'''
 I have saved the cube generated by this chunk of code as it was pretty slow
 Regions file downloaded from link in Stone paper 
'''
#file =  "/home/hh21501/farfrom/tmp/region_fx-WRAF2-v4-1_WRAF_All-Hist_est1_v4-1_4-1-0_000000-000000.nc"   # 68 regions
#regions = iris.load(file)
#reg_ind = regions[4]
#reg_grid = obs[0,:,:].copy()
#for i in np.arange(np.shape(reg_grid)[0]):
#   for j in np.arange(np.shape(reg_grid)[1]):
#       lat = reg_grid[i,j].coord('latitude').points[0]
#       lon = reg_grid[i,j].coord('longitude').points[0]
#       lat_index = np.argmin(np.abs(reg_ind.coord('latitude').points-lat))
#       lon_index = np.argmin(np.abs(reg_ind.coord('longitude').points-lon))
#       reg_grid.data[i, j] = reg_ind.data[lat_index, lon_index]
#
#iris.save(reg_grid, '/home/hh21501/farfrom/region_grid.nc')

reg_grid = iris.load_cube('/home/hh21501/farfrom/region_grid.nc')

'''
I was hoping by adding the regions as an aux_coord to the data I could then extract with a constraint, but I can't get it to work with a 2d constraint - I'm sure there is a way? 
'''
## Add regions as an aux_coord 
obs.add_aux_coord(iris.coords.AuxCoord(reg_grid.data, long_name='regions'), data_dims=[1, 2])



'''
This section of code calculates only the largest extreme in each region
Needs altering to identify every value then keep all those over 3.5std,
and also save the year they are from (maybe also month?)
Should just need to adjust the final line (far_from.append...)
The mask function is very slow and I'm sure could be improved (see comment about using constraint above)
'''
list_of_regions = np.arange[68] # I haven't checked that the regions are just labelled np.arange[68], as I just ran for a single region whilst testing. Needs checking!
far_from = []
for each in list_of_regions:
    masked = region_mask(obs, each)
    ## Lat and lon collapse
    reg = masked.collapsed(['latitude','longitude'], iris.analysis.MEAN) 
    ## Which month has max?
    max_index = np.where(reg.data == np.max(reg.data)) # identify index of maximum temp
    max_month = reg[max_index].coord('month_number').points[0]  # identify month_number of maximum temp
    ## Extract data for just that month
    month_cons = iris.Constraint(time=lambda cell: cell.point.month == max_month)
    reg = reg.extract(month_cons) # single month
    ## Calculate how far max is from mean, in terms of stdev
    far_from.append((np.max(reg.data) - np.mean(reg.data)) / np.std(reg.data))





## Plot (my usual plotting code, in case it's useful)
#cube = obs[0,:,:]
#fig, axes = plt.subplots \
#    (nrowsols=1, figsize=(7., 7.), dpi=80, num=None)
#brewer_cmap = mpl_cm.get_cmap('brewer_RdBu_11')
#ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=100))
#im = plt.pcolormesh(cube.coord('longitude').points, cube.coord      
#    ('latitude').points, \
#    cube.data, transform=ccrs.PlateCarree(), cmap=brewer_cmap)
#ax.coastlines()
#cbar_ax = fig.add_axes()
#cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')
#plt.savefig('test.png', dpi=300, bbox_inches='tight')
#plt.close()
